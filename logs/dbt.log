[0m23:59:33.377671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012BE7A2DD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012BEA2FB550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012BEA2FB3A0>]}


============================== 23:59:33.377671 | 1c316daf-709e-4ec9-8956-db407afb185f ==============================
[0m23:59:33.377671 [info ] [MainThread]: Running with dbt=1.8.4
[0m23:59:33.377671 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m23:59:33.421981 [info ] [MainThread]: dbt version: 1.8.4
[0m23:59:33.421981 [info ] [MainThread]: python version: 3.9.13
[0m23:59:33.421981 [info ] [MainThread]: python path: C:\Medallion-Spark-DBT\.venv\Scripts\python.exe
[0m23:59:33.421981 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m23:59:33.635527 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:59:33.635527 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:59:33.635527 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:59:36.261015 [info ] [MainThread]: Using profiles dir at C:\Users\User\.dbt
[0m23:59:36.276624 [info ] [MainThread]: Using profiles.yml file at C:\Users\User\.dbt\profiles.yml
[0m23:59:36.277970 [info ] [MainThread]: Using dbt_project.yml file at C:\Medallion-Spark-DBT\medallion_dbt_spark\dbt_project.yml
[0m23:59:36.277970 [info ] [MainThread]: adapter type: databricks
[0m23:59:36.284987 [info ] [MainThread]: adapter version: 1.8.4
[0m23:59:36.554994 [info ] [MainThread]: Configuration:
[0m23:59:36.554994 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m23:59:36.559426 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m23:59:36.559426 [info ] [MainThread]: Required dependencies:
[0m23:59:36.563444 [debug] [MainThread]: Executing "git --help"
[0m23:59:36.651471 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:59:36.651471 [debug] [MainThread]: STDERR: "b''"
[0m23:59:36.651471 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m23:59:36.651471 [info ] [MainThread]: Connection:
[0m23:59:36.659483 [info ] [MainThread]:   host: adb-42589726596291.11.azuredatabricks.net
[0m23:59:36.659483 [info ] [MainThread]:   http_path: sql/protocolv1/o/42589726596291/0804-154241-osij06b5
[0m23:59:36.663381 [info ] [MainThread]:   catalog: hive_metastore
[0m23:59:36.663381 [info ] [MainThread]:   schema: saleslt
[0m23:59:36.667912 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m23:59:36.667912 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1286430821824, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(10188, 6664), compute-name=) - Creating connection
[0m23:59:36.667912 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m23:59:36.667912 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1286430821824, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10188, 6664), compute-name=) - Acquired connection on thread (10188, 6664), using default compute resource
[0m23:59:36.667912 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1286430821824, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10188, 6664), compute-name=) - Checking idleness
[0m23:59:36.667912 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1286430821824, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10188, 6664), compute-name=) - Retrieving connection
[0m23:59:36.675951 [debug] [MainThread]: Using databricks connection "debug"
[0m23:59:36.675951 [debug] [MainThread]: On debug: select 1 as id
[0m23:59:36.675951 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:59:36.944085 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1286430821824, session-id=ef345c2b-1102-42a2-b818-84e70f7fde17, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(10188, 6664), compute-name=) - Connection created
[0m23:59:36.944085 [debug] [MainThread]: Databricks adapter: Cursor(session-id=ef345c2b-1102-42a2-b818-84e70f7fde17, command-id=Unknown) - Created cursor
[0m23:59:37.110280 [debug] [MainThread]: SQL status: OK in 0.4300000071525574 seconds
[0m23:59:37.110280 [debug] [MainThread]: Databricks adapter: Cursor(session-id=ef345c2b-1102-42a2-b818-84e70f7fde17, command-id=a1513b92-6cee-4075-9390-cd3e8a22dcfa) - Closing cursor
[0m23:59:37.110280 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1286430821824, session-id=ef345c2b-1102-42a2-b818-84e70f7fde17, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(10188, 6664), compute-name=) - Released connection
[0m23:59:37.110280 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m23:59:37.125658 [info ] [MainThread]: [32mAll checks passed![0m
[0m23:59:37.127693 [debug] [MainThread]: Command `dbt debug` succeeded at 23:59:37.127693 after 3.92 seconds
[0m23:59:37.133733 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m23:59:37.133733 [debug] [MainThread]: On debug: Close
[0m23:59:37.133733 [debug] [MainThread]: Databricks adapter: Connection(session-id=ef345c2b-1102-42a2-b818-84e70f7fde17) - Closing connection
[0m23:59:37.218182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012BE7A2DD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012BE5FD8FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012B854172B0>]}
[0m23:59:37.218182 [debug] [MainThread]: Flushing usage events
[0m00:20:39.086351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220CDA6CDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220D033B6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220D033B520>]}


============================== 00:20:39.117802 | 7be5758f-33e6-496a-8984-9af79b7be348 ==============================
[0m00:20:39.117802 [info ] [MainThread]: Running with dbt=1.8.4
[0m00:20:39.121349 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:20:44.416175 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:20:44.416175 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:20:44.419215 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:20:57.890353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7be5758f-33e6-496a-8984-9af79b7be348', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220EA46EC70>]}
[0m00:20:57.999746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7be5758f-33e6-496a-8984-9af79b7be348', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220E9F4C1C0>]}
[0m00:20:57.999746 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m00:20:58.064178 [debug] [MainThread]: checksum: 54398663eddc8e4ac172fdab397e09b2aa6984947a84e6ec7983096c3fae7b7a, vars: {}, profile: , target: , version: 1.8.4
[0m00:20:58.064178 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:20:58.064178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7be5758f-33e6-496a-8984-9af79b7be348', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220EA4BDDF0>]}
[0m00:21:04.440872 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.medallion_dbt_spark.address_snapshot' (snapshots\address.sql) depends on a source named 'saleslt.address' which was not found
[0m00:21:04.448966 [debug] [MainThread]: Command `dbt snapshot` failed at 00:21:04.447944 after 25.55 seconds
[0m00:21:04.448966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220CDA6CDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220EAF1E0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220EAF19550>]}
[0m00:21:04.448966 [debug] [MainThread]: Flushing usage events
[0m00:25:11.572601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220368EDD60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220391BB580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220391BB3D0>]}


============================== 00:25:11.572601 | 2f9143b9-450b-4821-a3aa-5588c2176832 ==============================
[0m00:25:11.572601 [info ] [MainThread]: Running with dbt=1.8.4
[0m00:25:11.588571 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:25:11.944169 [info ] [MainThread]: dbt version: 1.8.4
[0m00:25:11.944169 [info ] [MainThread]: python version: 3.9.13
[0m00:25:11.947229 [info ] [MainThread]: python path: C:\Medallion-Spark-DBT\.venv\Scripts\python.exe
[0m00:25:11.947229 [info ] [MainThread]: os info: Windows-10-10.0.22631-SP0
[0m00:25:12.150340 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:25:12.150340 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:25:12.150340 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:25:14.958138 [info ] [MainThread]: Using profiles dir at C:\Users\User\.dbt
[0m00:25:14.958138 [info ] [MainThread]: Using profiles.yml file at C:\Users\User\.dbt\profiles.yml
[0m00:25:14.970921 [info ] [MainThread]: Using dbt_project.yml file at C:\Medallion-Spark-DBT\medallion_dbt_spark\dbt_project.yml
[0m00:25:14.970921 [info ] [MainThread]: adapter type: databricks
[0m00:25:14.970921 [info ] [MainThread]: adapter version: 1.8.4
[0m00:25:15.256039 [info ] [MainThread]: Configuration:
[0m00:25:15.256039 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:25:15.261440 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:25:15.264454 [info ] [MainThread]: Required dependencies:
[0m00:25:15.264454 [debug] [MainThread]: Executing "git --help"
[0m00:25:15.352480 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:25:15.352480 [debug] [MainThread]: STDERR: "b''"
[0m00:25:15.360499 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:25:15.360499 [info ] [MainThread]: Connection:
[0m00:25:15.360499 [info ] [MainThread]:   host: adb-42589726596291.11.azuredatabricks.net
[0m00:25:15.368480 [info ] [MainThread]:   http_path: sql/protocolv1/o/42589726596291/0804-154241-osij06b5
[0m00:25:15.368480 [info ] [MainThread]:   catalog: hive_metastore
[0m00:25:15.368480 [info ] [MainThread]:   schema: saleslt
[0m00:25:15.368480 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m00:25:15.376501 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2337853174112, session-id=None, name=debug, idle-time=0s, acquire-count=0, language=None, thread-identifier=(14512, 11712), compute-name=) - Creating connection
[0m00:25:15.379640 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m00:25:15.379640 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2337853174112, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14512, 11712), compute-name=) - Acquired connection on thread (14512, 11712), using default compute resource
[0m00:25:15.379640 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2337853174112, session-id=None, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14512, 11712), compute-name=) - Checking idleness
[0m00:25:15.384658 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2337853174112, session-id=None, name=debug, idle-time=0.005017995834350586s, acquire-count=1, language=None, thread-identifier=(14512, 11712), compute-name=) - Retrieving connection
[0m00:25:15.384658 [debug] [MainThread]: Using databricks connection "debug"
[0m00:25:15.384658 [debug] [MainThread]: On debug: select 1 as id
[0m00:25:15.384658 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:25:16.888700 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2337853174112, session-id=f9fc8b1d-b998-4182-a86a-932945a9f59c, name=debug, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(14512, 11712), compute-name=) - Connection created
[0m00:25:16.888700 [debug] [MainThread]: Databricks adapter: Cursor(session-id=f9fc8b1d-b998-4182-a86a-932945a9f59c, command-id=Unknown) - Created cursor
[0m00:25:17.248064 [debug] [MainThread]: SQL status: OK in 1.8600000143051147 seconds
[0m00:25:17.264126 [debug] [MainThread]: Databricks adapter: Cursor(session-id=f9fc8b1d-b998-4182-a86a-932945a9f59c, command-id=1a10003d-0428-4869-bff3-a5cf7f4470da) - Closing cursor
[0m00:25:17.264126 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2337853174112, session-id=f9fc8b1d-b998-4182-a86a-932945a9f59c, name=debug, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(14512, 11712), compute-name=) - Released connection
[0m00:25:17.264126 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:25:17.264126 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:25:17.272152 [debug] [MainThread]: Command `dbt debug` succeeded at 00:25:17.272152 after 5.86 seconds
[0m00:25:17.272152 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:25:17.272152 [debug] [MainThread]: On debug: Close
[0m00:25:17.272152 [debug] [MainThread]: Databricks adapter: Connection(session-id=f9fc8b1d-b998-4182-a86a-932945a9f59c) - Closing connection
[0m00:25:17.362922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220368EDD60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022052E9FDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022034EA8FD0>]}
[0m00:25:17.368947 [debug] [MainThread]: Flushing usage events
[0m00:25:59.601925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227D27ADDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227D507B6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227D507B520>]}


============================== 00:25:59.617525 | 2089424c-3a9f-4c62-b596-2709697907ee ==============================
[0m00:25:59.617525 [info ] [MainThread]: Running with dbt=1.8.4
[0m00:25:59.617525 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m00:25:59.809447 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:25:59.809447 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:25:59.809447 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:26:02.205727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EEDD17C0>]}
[0m00:26:02.315098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227D4BFEA90>]}
[0m00:26:02.315098 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m00:26:02.345509 [debug] [MainThread]: checksum: 54398663eddc8e4ac172fdab397e09b2aa6984947a84e6ec7983096c3fae7b7a, vars: {}, profile: , target: , version: 1.8.4
[0m00:26:02.353284 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m00:26:02.353284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EECF6190>]}
[0m00:26:07.062887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFE53DC0>]}
[0m00:26:07.470119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFE5D610>]}
[0m00:26:07.470119 [info ] [MainThread]: Found 2 models, 7 snapshots, 4 data tests, 9 sources, 585 macros
[0m00:26:07.470119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFDF61C0>]}
[0m00:26:07.478158 [info ] [MainThread]: 
[0m00:26:07.478158 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(6448, 3996), compute-name=) - Creating connection
[0m00:26:07.478158 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:26:07.478158 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Acquired connection on thread (6448, 3996), using default compute resource
[0m00:26:07.494178 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(6448, 10744), compute-name=) - Creating connection
[0m00:26:07.502321 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:26:07.502321 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Acquired connection on thread (6448, 10744), using default compute resource
[0m00:26:07.502321 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Checking idleness
[0m00:26:07.502321 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Retrieving connection
[0m00:26:07.502321 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:26:07.502321 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:26:07.502321 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:26:07.848752 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Connection created
[0m00:26:07.848752 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, command-id=Unknown) - Created cursor
[0m00:26:08.186148 [debug] [ThreadPool]: SQL status: OK in 0.6800000071525574 seconds
[0m00:26:08.218181 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, command-id=83efa22b-f014-4c63-9465-55e8ed107162) - Closing cursor
[0m00:26:08.226142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(6448, 10744), compute-name=) - Released connection
[0m00:26:08.226142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=list_hive_metastore, idle-time=0.007960319519042969s, acquire-count=0, language=None, thread-identifier=(6448, 10744), compute-name=) - Checking idleness
[0m00:26:08.226142 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m00:26:08.226142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.007960319519042969s, acquire-count=0, language=None, thread-identifier=(6448, 10744), compute-name=) - Reusing connection previously named list_hive_metastore
[0m00:26:08.226142 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.007960319519042969s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Acquired connection on thread (6448, 10744), using default compute resource
[0m00:26:08.234137 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.015955448150634766s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Checking idleness
[0m00:26:08.234137 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.015955448150634766s, acquire-count=2, language=None, thread-identifier=(6448, 10744), compute-name=) - Acquired connection on thread (6448, 10744), using default compute resource
[0m00:26:08.234137 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m00:26:08.258173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.03999137878417969s, acquire-count=2, language=None, thread-identifier=(6448, 10744), compute-name=) - Checking idleness
[0m00:26:08.258173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.03999137878417969s, acquire-count=2, language=None, thread-identifier=(6448, 10744), compute-name=) - Retrieving connection
[0m00:26:08.258173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.03999137878417969s, acquire-count=2, language=None, thread-identifier=(6448, 10744), compute-name=) - Checking idleness
[0m00:26:08.266140 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.0479586124420166s, acquire-count=2, language=None, thread-identifier=(6448, 10744), compute-name=) - Retrieving connection
[0m00:26:08.266140 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:26:08.266140 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m00:26:08.266140 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m00:26:08.266140 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, command-id=Unknown) - Created cursor
[0m00:26:09.376213 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m00:26:09.376213 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, command-id=57cea03b-5887-48f9-9d48-f07c0aa0a2eb) - Closing cursor
[0m00:26:09.376213 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m00:26:09.376213 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=1.158031940460205s, acquire-count=1, language=None, thread-identifier=(6448, 10744), compute-name=) - Released connection
[0m00:26:09.376213 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370552198816, session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6, name=create_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(6448, 10744), compute-name=) - Released connection
[0m00:26:09.391585 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(6448, 11252), compute-name=) - Creating connection
[0m00:26:09.391585 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:26:09.391585 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Acquired connection on thread (6448, 11252), using default compute resource
[0m00:26:09.391585 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Checking idleness
[0m00:26:09.391585 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Retrieving connection
[0m00:26:09.391585 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:26:09.391585 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:26:09.391585 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:26:09.657197 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Connection created
[0m00:26:09.657197 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=Unknown) - Created cursor
[0m00:26:09.934517 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m00:26:09.950126 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=365aebe0-016a-4b42-b628-78df047c8987) - Closing cursor
[0m00:26:09.965760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.3085627555847168s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Checking idleness
[0m00:26:09.965760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.3085627555847168s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Retrieving connection
[0m00:26:09.965760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.3085627555847168s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Checking idleness
[0m00:26:09.965760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.3085627555847168s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Retrieving connection
[0m00:26:09.981360 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:26:09.981360 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:26:09.981360 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:26:09.981360 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=Unknown) - Created cursor
[0m00:26:10.231463 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m00:26:10.231463 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=0c632717-d44b-4aa5-aa64-5d5f9eca8f73) - Closing cursor
[0m00:26:10.247001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.58980393409729s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Checking idleness
[0m00:26:10.247001 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.58980393409729s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Retrieving connection
[0m00:26:10.247001 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:26:10.247001 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:26:10.247001 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=Unknown) - Created cursor
[0m00:26:10.531462 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m00:26:10.531462 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=fe2965c4-33c2-4678-94e8-88bea978266c) - Closing cursor
[0m00:26:10.531462 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(6448, 11252), compute-name=) - Released connection
[0m00:26:10.547123 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_saleslt, idle-time=0.015660762786865234s, acquire-count=0, language=None, thread-identifier=(6448, 11252), compute-name=) - Checking idleness
[0m00:26:10.547123 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:26:10.547123 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_snapshots, idle-time=0.015660762786865234s, acquire-count=0, language=None, thread-identifier=(6448, 11252), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m00:26:10.547123 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_snapshots, idle-time=0.015660762786865234s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Acquired connection on thread (6448, 11252), using default compute resource
[0m00:26:10.547123 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_snapshots, idle-time=0.015660762786865234s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Checking idleness
[0m00:26:10.547123 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_snapshots, idle-time=0.015660762786865234s, acquire-count=1, language=None, thread-identifier=(6448, 11252), compute-name=) - Retrieving connection
[0m00:26:10.547123 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:26:10.562741 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:26:10.562741 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=Unknown) - Created cursor
[0m00:26:10.750244 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m00:26:10.750244 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, command-id=063cd39c-b4c0-41a1-9694-bba7bbee4c63) - Closing cursor
[0m00:26:10.750244 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2370553316352, session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(6448, 11252), compute-name=) - Released connection
[0m00:26:10.750244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFD24940>]}
[0m00:26:10.750244 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=3.2720863819122314s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Checking idleness
[0m00:26:10.765873 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=3.287715196609497s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Retrieving connection
[0m00:26:10.765873 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=3.287715196609497s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Checking idleness
[0m00:26:10.765873 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=3.287715196609497s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Retrieving connection
[0m00:26:10.765873 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:26:10.765873 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:26:10.765873 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(6448, 3996), compute-name=) - Released connection
[0m00:26:10.765873 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:26:10.765873 [info ] [MainThread]: 
[0m00:26:10.784585 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m00:26:10.784585 [info ] [Thread-1  ]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m00:26:10.792587 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(6448, 13092), compute-name=) - Creating connection
[0m00:26:10.792587 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.medallion_dbt_spark.address_snapshot'
[0m00:26:10.792587 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m00:26:10.792587 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m00:26:10.816569 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m00:26:11.021727 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.address_snapshot"
[0m00:26:11.037331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.229140043258667s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:11.037331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.24474430084228516s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:11.037331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.24474430084228516s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:11.037331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.24474430084228516s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:11.037331 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:26:11.037331 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m00:26:11.037331 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ) sbq



  
      
[0m00:26:11.037331 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m00:26:11.297426 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Connection created
[0m00:26:11.297426 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:24.369026 [debug] [Thread-1  ]: SQL status: OK in 13.329999923706055 seconds
[0m00:26:24.369026 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=db66ee6a-6f3d-4b43-9115-9e6d60a31d24) - Closing cursor
[0m00:26:24.522794 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:24.538429 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:24.538429 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:24.538429 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227F0007DF0>]}
[0m00:26:24.538429 [info ] [Thread-1  ]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 13.75s]
[0m00:26:24.550516 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m00:26:24.550516 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:26:24.550516 [info ] [Thread-1  ]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m00:26:24.550516 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.01208639144897461s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:24.558545 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m00:26:24.558545 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.020115375518798828s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.address_snapshot
[0m00:26:24.558545 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.020115375518798828s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m00:26:24.558545 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:26:24.566540 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:26:24.590568 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.customer_snapshot"
[0m00:26:24.590568 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.052138566970825195s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:24.590568 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.052138566970825195s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:24.598550 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m00:26:24.598550 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ) sbq



  
      
[0m00:26:24.598550 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:29.114906 [debug] [Thread-1  ]: SQL status: OK in 4.519999980926514 seconds
[0m00:26:29.114906 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=e0d75a72-2fc4-4e94-bce4-a0b2942ec50e) - Closing cursor
[0m00:26:29.130565 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:29.130565 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:29.130565 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:29.130565 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227D3FEF640>]}
[0m00:26:29.130565 [info ] [Thread-1  ]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 4.58s]
[0m00:26:29.130565 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:26:29.146070 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:26:29.146070 [info ] [Thread-1  ]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m00:26:29.149801 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.019235610961914062s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:29.149801 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m00:26:29.154331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.023766040802001953s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customer_snapshot
[0m00:26:29.154331 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.023766040802001953s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m00:26:29.154331 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:26:29.170675 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:26:29.186691 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m00:26:29.186691 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.05612587928771973s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:29.194719 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.06415343284606934s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:29.194719 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m00:26:29.194719 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ) sbq



  
      
[0m00:26:29.194719 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:33.775600 [debug] [Thread-1  ]: SQL status: OK in 4.579999923706055 seconds
[0m00:26:33.775600 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=36ed9c70-4493-4bc7-af24-f0a9d369e3d2) - Closing cursor
[0m00:26:33.783583 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:33.783583 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:33.791612 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:33.791612 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFD1B820>]}
[0m00:26:33.791612 [info ] [Thread-1  ]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 4.64s]
[0m00:26:33.799582 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:26:33.799582 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m00:26:33.799582 [info ] [Thread-1  ]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m00:26:33.799582 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.007970809936523438s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:33.807592 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m00:26:33.807592 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.01598072052001953s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:26:33.807592 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.01598072052001953s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m00:26:33.807592 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m00:26:33.815585 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m00:26:33.839592 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.product_snapshot"
[0m00:26:33.839592 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.047980546951293945s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:33.839592 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.047980546951293945s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:33.839592 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m00:26:33.839592 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot

    ) sbq



  
      
[0m00:26:33.847592 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:37.918206 [debug] [Thread-1  ]: SQL status: OK in 4.070000171661377 seconds
[0m00:26:37.918206 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=c4e8216b-ba2c-4ecc-b15a-798d8fc67efa) - Closing cursor
[0m00:26:37.930325 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:37.934104 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.003779172897338867s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:37.936767 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:37.938419 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227F00C6400>]}
[0m00:26:37.938419 [info ] [Thread-1  ]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 4.14s]
[0m00:26:37.947552 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m00:26:37.947552 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:26:37.947552 [info ] [Thread-1  ]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m00:26:37.955580 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.01881241798400879s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:37.955580 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m00:26:37.955580 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.01881241798400879s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.product_snapshot
[0m00:26:37.963598 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.026830673217773438s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m00:26:37.963598 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:26:37.979582 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:26:37.998805 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m00:26:38.003830 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0670626163482666s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:38.003830 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0670626163482666s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:38.011875 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m00:26:38.011875 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot

    ) sbq



  
      
[0m00:26:38.011875 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:42.561714 [debug] [Thread-1  ]: SQL status: OK in 4.550000190734863 seconds
[0m00:26:42.567737 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=43caab4c-8f42-4c11-8dd1-afb3a9656047) - Closing cursor
[0m00:26:42.572872 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:42.572872 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:42.577165 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:42.580670 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227F00C6FA0>]}
[0m00:26:42.585304 [info ] [Thread-1  ]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 4.62s]
[0m00:26:42.588685 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:26:42.594565 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:26:42.598868 [info ] [Thread-1  ]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m00:26:42.609680 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.031949758529663086s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:42.609680 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m00:26:42.618460 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.041559457778930664s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:26:42.620651 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.04375052452087402s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m00:26:42.620651 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:26:42.650704 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:26:42.698825 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m00:26:42.704401 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.1275010108947754s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:42.707932 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.1275010108947754s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:42.710058 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m00:26:42.710058 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot

    ) sbq



  
      
[0m00:26:42.710058 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:47.033226 [debug] [Thread-1  ]: SQL status: OK in 4.320000171661377 seconds
[0m00:26:47.033226 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=bb282f31-87ee-418c-b7e2-7457acd5d819) - Closing cursor
[0m00:26:47.039768 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:47.041654 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:47.041654 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:47.041654 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227F0113C40>]}
[0m00:26:47.048180 [info ] [Thread-1  ]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 4.43s]
[0m00:26:47.048991 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:26:47.048991 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:26:47.048991 [info ] [Thread-1  ]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m00:26:47.059799 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.018144607543945312s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:47.059799 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m00:26:47.059799 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.018144607543945312s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:26:47.065323 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.018144607543945312s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Acquired connection on thread (6448, 13092), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m00:26:47.067847 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:26:47.078715 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:26:47.090295 [debug] [Thread-1  ]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m00:26:47.097811 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.048641204833984375s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Checking idleness
[0m00:26:47.098976 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.05732226371765137s, acquire-count=1, language=sql, thread-identifier=(6448, 13092), compute-name=) - Retrieving connection
[0m00:26:47.098976 [debug] [Thread-1  ]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m00:26:47.098976 [debug] [Thread-1  ]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot

    ) sbq



  
      
[0m00:26:47.105994 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=Unknown) - Created cursor
[0m00:26:51.069742 [debug] [Thread-1  ]: SQL status: OK in 3.9600000381469727 seconds
[0m00:26:51.069742 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, command-id=c9ff2d7f-9d38-470d-b6d9-28c2fdfc315e) - Closing cursor
[0m00:26:51.078829 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m00:26:51.082890 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:51.088576 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2370553544512, session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(6448, 13092), compute-name=) - Released connection
[0m00:26:51.088576 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2089424c-3a9f-4c62-b596-2709697907ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227F001DEB0>]}
[0m00:26:51.092383 [info ] [Thread-1  ]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 4.03s]
[0m00:26:51.101697 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:26:51.108161 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=40.33609414100647s, acquire-count=0, language=None, thread-identifier=(6448, 3996), compute-name=) - Checking idleness
[0m00:26:51.108680 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=40.342806816101074s, acquire-count=0, language=None, thread-identifier=(6448, 3996), compute-name=) - Reusing connection previously named master
[0m00:26:51.112753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=40.346879720687866s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Acquired connection on thread (6448, 3996), using default compute resource
[0m00:26:51.112753 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=40.346879720687866s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Checking idleness
[0m00:26:51.118510 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=None, name=master, idle-time=40.35263657569885s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Retrieving connection
[0m00:26:51.119671 [debug] [MainThread]: On master: ROLLBACK
[0m00:26:51.119671 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:26:51.452299 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=1f099afa-9e2c-4afc-aa32-6f09a2e4755f, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Connection created
[0m00:26:51.452299 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:26:51.458905 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=1f099afa-9e2c-4afc-aa32-6f09a2e4755f, name=master, idle-time=0.006606101989746094s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Checking idleness
[0m00:26:51.460257 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=1f099afa-9e2c-4afc-aa32-6f09a2e4755f, name=master, idle-time=0.007957220077514648s, acquire-count=1, language=None, thread-identifier=(6448, 3996), compute-name=) - Retrieving connection
[0m00:26:51.460257 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:26:51.466600 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:26:51.468723 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2370551219728, session-id=1f099afa-9e2c-4afc-aa32-6f09a2e4755f, name=master, idle-time=0.0005362033843994141s, acquire-count=0, language=None, thread-identifier=(6448, 3996), compute-name=) - Released connection
[0m00:26:51.468723 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:26:51.468723 [debug] [MainThread]: On master: ROLLBACK
[0m00:26:51.468723 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:26:51.476593 [debug] [MainThread]: On master: Close
[0m00:26:51.478675 [debug] [MainThread]: Databricks adapter: Connection(session-id=1f099afa-9e2c-4afc-aa32-6f09a2e4755f) - Closing connection
[0m00:26:51.598801 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m00:26:51.598801 [debug] [MainThread]: On create_hive_metastore_snapshots: ROLLBACK
[0m00:26:51.598801 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:26:51.605827 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m00:26:51.608866 [debug] [MainThread]: Databricks adapter: Connection(session-id=51e34a3b-a17f-4274-9736-cf99581fd4c6) - Closing connection
[0m00:26:51.759578 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:26:51.761673 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:26:51.761673 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:26:51.761673 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:26:51.768888 [debug] [MainThread]: Databricks adapter: Connection(session-id=7e0bfe4c-2bab-48c0-9a9f-da2eebf6aa90) - Closing connection
[0m00:26:51.851039 [debug] [MainThread]: Connection 'snapshot.medallion_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m00:26:51.858687 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: ROLLBACK
[0m00:26:51.859521 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:26:51.859521 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: Close
[0m00:26:51.859521 [debug] [MainThread]: Databricks adapter: Connection(session-id=8d3f6c2f-b3df-41a8-84f8-65a8831d235f) - Closing connection
[0m00:26:51.969167 [info ] [MainThread]: 
[0m00:26:51.969167 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 44.49 seconds (44.49s).
[0m00:26:51.978794 [debug] [MainThread]: Command end result
[0m00:26:52.128763 [info ] [MainThread]: 
[0m00:26:52.133865 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:26:52.137877 [info ] [MainThread]: 
[0m00:26:52.140426 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m00:26:52.148067 [debug] [MainThread]: Command `dbt snapshot` succeeded at 00:26:52.140426 after 52.68 seconds
[0m00:26:52.149401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227D27ADDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFE24280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227EFDDFFD0>]}
[0m00:26:52.149401 [debug] [MainThread]: Flushing usage events
[0m01:07:54.190195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA06EADDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA0977A760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA0977A5B0>]}


============================== 01:07:54.198210 | 78bdd72b-9a70-498f-93c5-cf3c0f910b1f ==============================
[0m01:07:54.198210 [info ] [MainThread]: Running with dbt=1.8.4
[0m01:07:54.206233 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:07:54.407743 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:07:54.407743 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:07:54.415743 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:07:57.752157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '78bdd72b-9a70-498f-93c5-cf3c0f910b1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2369DC10>]}
[0m01:07:57.872137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '78bdd72b-9a70-498f-93c5-cf3c0f910b1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA088FB850>]}
[0m01:07:57.873606 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m01:07:57.902150 [debug] [MainThread]: checksum: 54398663eddc8e4ac172fdab397e09b2aa6984947a84e6ec7983096c3fae7b7a, vars: {}, profile: , target: , version: 1.8.4
[0m01:07:58.681259 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 7 files added, 0 files changed.
[0m01:07:58.683230 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\customer\dim_customer.yml
[0m01:07:58.684779 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\product\dim_product.sql
[0m01:07:58.684779 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\staging\bronze.yml
[0m01:07:58.684779 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m01:07:58.684779 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\sales\sales.yml
[0m01:07:58.684779 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m01:07:58.691295 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\sales\sales.sql
[0m01:07:58.692879 [debug] [MainThread]: Partial parsing: deleted file: medallion_dbt_spark://models\example\my_second_dbt_model.sql
[0m01:07:58.695238 [debug] [MainThread]: Partial parsing: deleted file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m01:07:59.762137 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `tests` config has been renamed to `data_tests`. Please see
https://docs.getdbt.com/docs/build/data-tests#new-data_tests-syntax for more
information.
[0m01:07:59.762137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '78bdd72b-9a70-498f-93c5-cf3c0f910b1f', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA243425E0>]}
[0m01:07:59.793288 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m01:08:00.223793 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m01:08:00.342000 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m01:08:00.381883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:08:00.381883 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:08:00.389667 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:08:00.392244 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:08:00.394377 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.397916 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.401432 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.401966 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.401966 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.406209 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.406209 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.413368 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.421701 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.422166 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.432206 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.433522 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.433522 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.439072 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.442169 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.442169 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.447139 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.447139 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.453559 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:08:00.455619 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:08:00.455619 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:08:00.461131 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:08:00.463730 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:08:00.662074 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.medallion_dbt_spark.example
[0m01:08:00.694760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '78bdd72b-9a70-498f-93c5-cf3c0f910b1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA24572F40>]}
[0m01:08:01.182097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '78bdd72b-9a70-498f-93c5-cf3c0f910b1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA24734DC0>]}
[0m01:08:01.183635 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 585 macros
[0m01:08:01.183635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '78bdd72b-9a70-498f-93c5-cf3c0f910b1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2450E520>]}
[0m01:08:01.191228 [info ] [MainThread]: 
[0m01:08:01.192262 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m01:08:01.195359 [debug] [MainThread]: Command end result
[0m01:08:01.292169 [debug] [MainThread]: Command `dbt test` succeeded at 01:08:01.292169 after 7.25 seconds
[0m01:08:01.295282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA06EADDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA092A3FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA2433D340>]}
[0m01:08:01.296802 [debug] [MainThread]: Flushing usage events
[0m01:09:45.625721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3439EDDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3462BA760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3462BA5B0>]}


============================== 01:09:45.636596 | 96bbcf49-fcd0-44d2-97ca-d1ccf6003947 ==============================
[0m01:09:45.636596 [info ] [MainThread]: Running with dbt=1.8.4
[0m01:09:45.638736 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:09:45.846495 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:09:45.846495 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:09:45.855024 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:09:48.385489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A34628E760>]}
[0m01:09:48.495487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A345BE3F40>]}
[0m01:09:48.495487 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m01:09:48.523728 [debug] [MainThread]: checksum: 54398663eddc8e4ac172fdab397e09b2aa6984947a84e6ec7983096c3fae7b7a, vars: {}, profile: , target: , version: 1.8.4
[0m01:09:48.846075 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m01:09:48.846075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3451C3FD0>]}
[0m01:09:52.778433 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `tests` config has been renamed to `data_tests`. Please see
https://docs.getdbt.com/docs/build/data-tests#new-data_tests-syntax for more
information.
[0m01:09:52.782047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A360E150D0>]}
[0m01:09:52.810625 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m01:09:53.015803 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\marts\product\dim_product.yml'
[0m01:09:53.059125 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\marts\sales\sales.yml'
[0m01:09:53.320842 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:09:53.320842 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:09:53.336534 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:09:53.336534 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_AddressId.86b771f63e' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m01:09:53.336534 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:09:53.344550 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:09:53.344550 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_product_name.991aec73f3' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:09:53.344550 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\marts\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m01:09:53.344550 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.352562 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.352562 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.352562 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.360544 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderQty.66af966596' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.360544 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productID.cbf6d34890' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.360544 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.368541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.368541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_name.4c7b961f77' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.368541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.368541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.376541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.376541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.376541 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.384570 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_customerID.60b0993af5' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.384570 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.384570 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.392544 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_freight.ca13e04131' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.392544 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_sales_totalDue.920571e023' (models\marts\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m01:09:53.654022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A36111A130>]}
[0m01:09:53.997777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A36135EA00>]}
[0m01:09:53.997777 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 585 macros
[0m01:09:54.013365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96bbcf49-fcd0-44d2-97ca-d1ccf6003947', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3610FAD90>]}
[0m01:09:54.015159 [info ] [MainThread]: 
[0m01:09:54.015159 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m01:09:54.021690 [debug] [MainThread]: Command end result
[0m01:09:54.109979 [debug] [MainThread]: Command `dbt test` succeeded at 01:09:54.109979 after 8.66 seconds
[0m01:09:54.109979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3439EDDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A3610FA820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A345BE3F40>]}
[0m01:09:54.109979 [debug] [MainThread]: Flushing usage events
[0m01:10:15.818213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC5D7ACE20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC60079760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC600795B0>]}


============================== 01:10:15.829627 | b52c17fb-409f-485f-a8d1-7500e77e102a ==============================
[0m01:10:15.829627 [info ] [MainThread]: Running with dbt=1.8.4
[0m01:10:15.834008 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:10:16.043484 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:10:16.043484 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:10:16.048117 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:10:18.658460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC606419A0>]}
[0m01:10:18.782488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC79BF0FA0>]}
[0m01:10:18.782488 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m01:10:18.808013 [debug] [MainThread]: checksum: 54398663eddc8e4ac172fdab397e09b2aa6984947a84e6ec7983096c3fae7b7a, vars: {}, profile: , target: , version: 1.8.4
[0m01:10:19.338120 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:10:19.338120 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:10:19.491918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC7AB4E130>]}
[0m01:10:19.837277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC7AB308B0>]}
[0m01:10:19.838313 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 585 macros
[0m01:10:19.839607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC7AB41940>]}
[0m01:10:19.842684 [info ] [MainThread]: 
[0m01:10:19.848290 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8448, 1728), compute-name=) - Creating connection
[0m01:10:19.848290 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:10:19.850832 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Acquired connection on thread (8448, 1728), using default compute resource
[0m01:10:19.868343 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115182722832, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8448, 3644), compute-name=) - Creating connection
[0m01:10:19.868343 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m01:10:19.868343 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115182722832, session-id=None, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 3644), compute-name=) - Acquired connection on thread (8448, 3644), using default compute resource
[0m01:10:19.872385 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115182722832, session-id=None, name=list_hive_metastore, idle-time=0.004042387008666992s, acquire-count=1, language=None, thread-identifier=(8448, 3644), compute-name=) - Checking idleness
[0m01:10:19.873251 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115182722832, session-id=None, name=list_hive_metastore, idle-time=0.004907846450805664s, acquire-count=1, language=None, thread-identifier=(8448, 3644), compute-name=) - Retrieving connection
[0m01:10:19.873251 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m01:10:19.877288 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m01:10:19.878402 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:10:20.578413 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115182722832, session-id=c1d917ed-64cd-43d9-baf3-6d1f56fc4a4d, name=list_hive_metastore, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 3644), compute-name=) - Connection created
[0m01:10:20.579773 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c1d917ed-64cd-43d9-baf3-6d1f56fc4a4d, command-id=Unknown) - Created cursor
[0m01:10:21.259427 [debug] [ThreadPool]: SQL status: OK in 1.3799999952316284 seconds
[0m01:10:21.289782 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c1d917ed-64cd-43d9-baf3-6d1f56fc4a4d, command-id=02337392-e109-42fb-be52-3583e98c38c8) - Closing cursor
[0m01:10:21.295480 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115182722832, session-id=c1d917ed-64cd-43d9-baf3-6d1f56fc4a4d, name=list_hive_metastore, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8448, 3644), compute-name=) - Released connection
[0m01:10:21.300013 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8448, 4864), compute-name=) - Creating connection
[0m01:10:21.303030 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:10:21.303030 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Acquired connection on thread (8448, 4864), using default compute resource
[0m01:10:21.303030 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:21.307552 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:21.308092 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:10:21.308092 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:10:21.308092 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:10:21.628765 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Connection created
[0m01:10:21.637317 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=Unknown) - Created cursor
[0m01:10:21.908329 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m01:10:21.918341 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=ed0a51ab-1628-4532-be6d-201ae328fb13) - Closing cursor
[0m01:10:21.950799 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.3220341205596924s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:21.950799 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.3220341205596924s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:21.950799 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.3220341205596924s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:21.958408 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.32857728004455566s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:21.959803 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:10:21.959803 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:10:21.959803 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m01:10:21.959803 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=Unknown) - Created cursor
[0m01:10:22.190022 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m01:10:22.198171 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=950650f2-dc02-44b5-a2c7-faca766d4389) - Closing cursor
[0m01:10:22.214206 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.5854418277740479s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:22.214206 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.5854418277740479s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:22.217334 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:10:22.218679 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m01:10:22.218679 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=Unknown) - Created cursor
[0m01:10:22.399919 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m01:10:22.408019 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=07b7055b-72e1-4a3f-b085-c59b0b69cb84) - Closing cursor
[0m01:10:22.412133 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8448, 4864), compute-name=) - Released connection
[0m01:10:22.412133 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:22.419823 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m01:10:22.419823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.007689952850341797s, acquire-count=0, language=None, thread-identifier=(8448, 4864), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m01:10:22.419823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.007689952850341797s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Acquired connection on thread (8448, 4864), using default compute resource
[0m01:10:22.419823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.007689952850341797s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:22.419823 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.007689952850341797s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:22.419823 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:10:22.419823 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m01:10:22.428373 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=Unknown) - Created cursor
[0m01:10:22.562321 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m01:10:22.569916 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=70e547c2-5710-48a1-90cc-08edc0ef0026) - Closing cursor
[0m01:10:22.580702 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.16634798049926758s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:22.580702 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.16856861114501953s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:22.580702 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:10:22.580702 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m01:10:22.580702 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=Unknown) - Created cursor
[0m01:10:22.719951 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m01:10:22.727783 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=bafb509f-a302-40c8-9437-ea410712f7af) - Closing cursor
[0m01:10:22.733261 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.32112765312194824s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Checking idleness
[0m01:10:22.733261 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.32112765312194824s, acquire-count=1, language=None, thread-identifier=(8448, 4864), compute-name=) - Retrieving connection
[0m01:10:22.737367 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:10:22.738403 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m01:10:22.740546 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=Unknown) - Created cursor
[0m01:10:22.968308 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m01:10:22.979787 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, command-id=9cae2590-212b-4408-83a5-368218ba6d7c) - Closing cursor
[0m01:10:22.981369 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=2115183204480, session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8448, 4864), compute-name=) - Released connection
[0m01:10:22.981369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC60017F40>]}
[0m01:10:22.988468 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=3.137636661529541s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Checking idleness
[0m01:10:22.989535 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=3.1387031078338623s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Retrieving connection
[0m01:10:22.989535 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=3.1387031078338623s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Checking idleness
[0m01:10:22.989535 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=3.1387031078338623s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Retrieving connection
[0m01:10:22.989535 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:10:22.989535 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:10:22.989535 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8448, 1728), compute-name=) - Released connection
[0m01:10:22.998373 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:10:23.000164 [info ] [MainThread]: 
[0m01:10:23.010949 [debug] [Thread-1  ]: Began running node model.medallion_dbt_spark.dim_customer
[0m01:10:23.014003 [info ] [Thread-1  ]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m01:10:23.018091 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0s, acquire-count=0, language=None, thread-identifier=(8448, 14908), compute-name=) - Creating connection
[0m01:10:23.019750 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m01:10:23.022388 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Acquired connection on thread (8448, 14908), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m01:10:23.023357 [debug] [Thread-1  ]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m01:10:23.046976 [debug] [Thread-1  ]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m01:10:23.048534 [debug] [Thread-1  ]: Began executing node model.medallion_dbt_spark.dim_customer
[0m01:10:23.079560 [debug] [Thread-1  ]: MATERIALIZING TABLE
[0m01:10:23.189848 [debug] [Thread-1  ]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m01:10:23.194730 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.16745972633361816s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Checking idleness
[0m01:10:23.194730 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.17234134674072266s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Retrieving connection
[0m01:10:23.197767 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.17234134674072266s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Checking idleness
[0m01:10:23.198326 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.17593741416931152s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Retrieving connection
[0m01:10:23.198326 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m01:10:23.198326 [debug] [Thread-1  ]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m01:10:23.203488 [debug] [Thread-1  ]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      

with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m01:10:23.203488 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m01:10:23.538376 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Connection created
[0m01:10:23.541662 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, command-id=Unknown) - Created cursor
[0m01:10:29.968139 [debug] [Thread-1  ]: SQL status: OK in 6.760000228881836 seconds
[0m01:10:29.974231 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, command-id=bc45425a-a018-40e8-8fd8-b964e84848ab) - Closing cursor
[0m01:10:30.198160 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Released connection
[0m01:10:30.198160 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Released connection
[0m01:10:30.204162 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC79808D60>]}
[0m01:10:30.207757 [info ] [Thread-1  ]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 7.18s]
[0m01:10:30.209868 [debug] [Thread-1  ]: Finished running node model.medallion_dbt_spark.dim_customer
[0m01:10:30.209868 [debug] [Thread-1  ]: Began running node model.medallion_dbt_spark.dim_product
[0m01:10:30.212436 [info ] [Thread-1  ]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m01:10:30.218002 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_customer, idle-time=0.019296646118164062s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Checking idleness
[0m01:10:30.218002 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m01:10:30.220565 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.022404909133911133s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_customer
[0m01:10:30.220565 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.022404909133911133s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Acquired connection on thread (8448, 14908), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m01:10:30.220565 [debug] [Thread-1  ]: Began compiling node model.medallion_dbt_spark.dim_product
[0m01:10:30.457865 [debug] [Thread-1  ]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m01:10:30.458380 [debug] [Thread-1  ]: Began executing node model.medallion_dbt_spark.dim_product
[0m01:10:30.468146 [debug] [Thread-1  ]: MATERIALIZING TABLE
[0m01:10:30.474645 [debug] [Thread-1  ]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m01:10:30.478228 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.28006815910339355s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Checking idleness
[0m01:10:30.478228 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.28006815910339355s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Retrieving connection
[0m01:10:30.478228 [debug] [Thread-1  ]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m01:10:30.482494 [debug] [Thread-1  ]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      

with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m01:10:30.482494 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, command-id=Unknown) - Created cursor
[0m01:10:34.641051 [debug] [Thread-1  ]: SQL status: OK in 4.159999847412109 seconds
[0m01:10:34.649069 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, command-id=860b1a41-f896-4165-890b-f95bb464aae5) - Closing cursor
[0m01:10:34.656366 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Released connection
[0m01:10:34.658471 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Released connection
[0m01:10:34.658471 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC7AC81F40>]}
[0m01:10:34.658471 [info ] [Thread-1  ]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 4.45s]
[0m01:10:34.664390 [debug] [Thread-1  ]: Finished running node model.medallion_dbt_spark.dim_product
[0m01:10:34.664390 [debug] [Thread-1  ]: Began running node model.medallion_dbt_spark.sales
[0m01:10:34.669473 [info ] [Thread-1  ]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m01:10:34.673021 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.dim_product, idle-time=0.014550447463989258s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Checking idleness
[0m01:10:34.673021 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.sales)
[0m01:10:34.673021 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.sales, idle-time=0.014550447463989258s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_product
[0m01:10:34.678117 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.sales, idle-time=0.019086837768554688s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Acquired connection on thread (8448, 14908), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m01:10:34.678117 [debug] [Thread-1  ]: Began compiling node model.medallion_dbt_spark.sales
[0m01:10:34.689462 [debug] [Thread-1  ]: Writing injected SQL for node "model.medallion_dbt_spark.sales"
[0m01:10:34.689462 [debug] [Thread-1  ]: Began executing node model.medallion_dbt_spark.sales
[0m01:10:34.698162 [debug] [Thread-1  ]: MATERIALIZING TABLE
[0m01:10:34.709291 [debug] [Thread-1  ]: Writing runtime sql for node "model.medallion_dbt_spark.sales"
[0m01:10:34.713344 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.sales, idle-time=0.05487322807312012s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Checking idleness
[0m01:10:34.717565 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.sales, idle-time=0.05487322807312012s, acquire-count=1, language=sql, thread-identifier=(8448, 14908), compute-name=) - Retrieving connection
[0m01:10:34.717976 [debug] [Thread-1  ]: Using databricks connection "model.medallion_dbt_spark.sales"
[0m01:10:34.722441 [debug] [Thread-1  ]: On model.medallion_dbt_spark.sales: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      

with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m01:10:34.722441 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, command-id=Unknown) - Created cursor
[0m01:10:39.600666 [debug] [Thread-1  ]: SQL status: OK in 4.880000114440918 seconds
[0m01:10:39.607215 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, command-id=68ce207b-b949-458c-b96f-c4ab89d76f18) - Closing cursor
[0m01:10:39.608280 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Released connection
[0m01:10:39.608280 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=2115184262496, session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(8448, 14908), compute-name=) - Released connection
[0m01:10:39.617474 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b52c17fb-409f-485f-a8d1-7500e77e102a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC79BE04C0>]}
[0m01:10:39.619674 [info ] [Thread-1  ]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 4.94s]
[0m01:10:39.619674 [debug] [Thread-1  ]: Finished running node model.medallion_dbt_spark.sales
[0m01:10:39.627287 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=16.637752056121826s, acquire-count=0, language=None, thread-identifier=(8448, 1728), compute-name=) - Checking idleness
[0m01:10:39.628344 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=16.638808727264404s, acquire-count=0, language=None, thread-identifier=(8448, 1728), compute-name=) - Reusing connection previously named master
[0m01:10:39.628344 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=16.638808727264404s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Acquired connection on thread (8448, 1728), using default compute resource
[0m01:10:39.628344 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=16.638808727264404s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Checking idleness
[0m01:10:39.633036 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=None, name=master, idle-time=16.64350152015686s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Retrieving connection
[0m01:10:39.637554 [debug] [MainThread]: On master: ROLLBACK
[0m01:10:39.638120 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:10:39.938220 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=df13ba10-b58f-4a52-8709-14d02bcb3353, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Connection created
[0m01:10:39.939704 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:10:39.939704 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=df13ba10-b58f-4a52-8709-14d02bcb3353, name=master, idle-time=0.002019643783569336s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Checking idleness
[0m01:10:39.941646 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=df13ba10-b58f-4a52-8709-14d02bcb3353, name=master, idle-time=0.003961324691772461s, acquire-count=1, language=None, thread-identifier=(8448, 1728), compute-name=) - Retrieving connection
[0m01:10:39.941646 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:10:39.941646 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:10:39.941646 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=2115182512544, session-id=df13ba10-b58f-4a52-8709-14d02bcb3353, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(8448, 1728), compute-name=) - Released connection
[0m01:10:39.947688 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:10:39.948227 [debug] [MainThread]: On master: ROLLBACK
[0m01:10:39.950361 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:10:39.950361 [debug] [MainThread]: On master: Close
[0m01:10:39.950361 [debug] [MainThread]: Databricks adapter: Connection(session-id=df13ba10-b58f-4a52-8709-14d02bcb3353) - Closing connection
[0m01:10:40.069673 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m01:10:40.072722 [debug] [MainThread]: On list_hive_metastore: Close
[0m01:10:40.072722 [debug] [MainThread]: Databricks adapter: Connection(session-id=c1d917ed-64cd-43d9-baf3-6d1f56fc4a4d) - Closing connection
[0m01:10:40.230478 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m01:10:40.237206 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m01:10:40.238250 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:10:40.239534 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m01:10:40.239534 [debug] [MainThread]: Databricks adapter: Connection(session-id=4ff253ed-dfb4-450a-94d0-e9f78fea55af) - Closing connection
[0m01:10:40.417502 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.sales' was properly closed.
[0m01:10:40.418048 [debug] [MainThread]: On model.medallion_dbt_spark.sales: ROLLBACK
[0m01:10:40.419607 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:10:40.419607 [debug] [MainThread]: On model.medallion_dbt_spark.sales: Close
[0m01:10:40.419607 [debug] [MainThread]: Databricks adapter: Connection(session-id=7c96eab7-6da4-46a0-b5fd-d756f092ea76) - Closing connection
[0m01:10:40.563165 [info ] [MainThread]: 
[0m01:10:40.564206 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 20.72 seconds (20.72s).
[0m01:10:40.568280 [debug] [MainThread]: Command end result
[0m01:10:40.687581 [info ] [MainThread]: 
[0m01:10:40.689525 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:10:40.689525 [info ] [MainThread]: 
[0m01:10:40.694465 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m01:10:40.698063 [debug] [MainThread]: Command `dbt run` succeeded at 01:10:40.698063 after 25.03 seconds
[0m01:10:40.702627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC5D7ACE20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC7AB308B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC7AB6E8E0>]}
[0m01:10:40.702627 [debug] [MainThread]: Flushing usage events
[0m01:14:02.354277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFC4EDDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFEDBC8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFEDBC730>]}


============================== 01:14:02.365149 | fdabe85d-0af9-4eb0-b6af-0de8356b680d ==============================
[0m01:14:02.365149 [info ] [MainThread]: Running with dbt=1.8.4
[0m01:14:02.367950 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt docs generate', 'send_anonymous_usage_stats': 'True'}
[0m01:14:02.603099 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:14:02.604355 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:14:02.604355 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:14:06.424463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fdabe85d-0af9-4eb0-b6af-0de8356b680d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFEB9D4F0>]}
[0m01:14:06.583109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fdabe85d-0af9-4eb0-b6af-0de8356b680d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFED7A610>]}
[0m01:14:06.589011 [info ] [MainThread]: Registered adapter: databricks=1.8.4
[0m01:14:06.623107 [debug] [MainThread]: checksum: 54398663eddc8e4ac172fdab397e09b2aa6984947a84e6ec7983096c3fae7b7a, vars: {}, profile: , target: , version: 1.8.4
[0m01:14:07.179146 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:14:07.182765 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:14:07.345485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fdabe85d-0af9-4eb0-b6af-0de8356b680d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B99D93E50>]}
[0m01:14:07.426976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fdabe85d-0af9-4eb0-b6af-0de8356b680d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B99DB85E0>]}
[0m01:14:07.426976 [info ] [MainThread]: Found 3 models, 7 snapshots, 9 sources, 585 macros
[0m01:14:07.426976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdabe85d-0af9-4eb0-b6af-0de8356b680d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B99DB8580>]}
[0m01:14:07.435106 [info ] [MainThread]: 
[0m01:14:07.435106 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1355495999472, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(9212, 3852), compute-name=) - Creating connection
[0m01:14:07.435106 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:14:07.435106 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1355495999472, session-id=None, name=master, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 3852), compute-name=) - Acquired connection on thread (9212, 3852), using default compute resource
[0m01:14:07.453581 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, acquire-count=0, language=None, thread-identifier=(9212, 12600), compute-name=) - Creating connection
[0m01:14:07.459118 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m01:14:07.462732 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Acquired connection on thread (9212, 12600), using default compute resource
[0m01:14:07.462732 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0005657672882080078s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:07.462732 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=None, name=list_hive_metastore_saleslt, idle-time=0.0005657672882080078s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:07.467261 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:14:07.467583 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m01:14:07.467583 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:14:07.842283 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Connection created
[0m01:14:07.842858 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=Unknown) - Created cursor
[0m01:14:08.482948 [debug] [ThreadPool]: SQL status: OK in 1.0199999809265137 seconds
[0m01:14:08.502951 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=8d40b08f-1a07-4d42-8ec9-623cbe53aba2) - Closing cursor
[0m01:14:08.542760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.7015242576599121s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:08.542760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.7015242576599121s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:08.542760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.7015242576599121s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:08.542760 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.7015242576599121s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:08.549018 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:14:08.549018 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:14:08.549018 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m01:14:08.552054 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=Unknown) - Created cursor
[0m01:14:08.832747 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m01:14:08.834227 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=c29f76ef-b40c-4641-b6a4-7d8218158ee3) - Closing cursor
[0m01:14:08.853229 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=1.010927438735962s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:08.854793 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=1.0135002136230469s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:08.854793 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m01:14:08.854793 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m01:14:08.854793 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=Unknown) - Created cursor
[0m01:14:09.130749 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m01:14:09.134284 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=0b3d9abc-e9d1-49e5-a45a-ebdbe49e4cdb) - Closing cursor
[0m01:14:09.138884 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(9212, 12600), compute-name=) - Released connection
[0m01:14:09.143100 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_saleslt, idle-time=0.004215240478515625s, acquire-count=0, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:09.148386 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m01:14:09.152400 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.009501934051513672s, acquire-count=0, language=None, thread-identifier=(9212, 12600), compute-name=) - Reusing connection previously named list_hive_metastore_saleslt
[0m01:14:09.152965 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.014080524444580078s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Acquired connection on thread (9212, 12600), using default compute resource
[0m01:14:09.154414 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.015529632568359375s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:09.155459 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.016574621200561523s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:09.155459 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:14:09.155459 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m01:14:09.155459 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=Unknown) - Created cursor
[0m01:14:09.383176 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m01:14:09.392847 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=5da23aed-9458-46e1-bba6-e54217619f5f) - Closing cursor
[0m01:14:09.402133 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.2632484436035156s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:09.403185 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.2643008232116699s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:09.405245 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:14:09.405245 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m01:14:09.405245 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=Unknown) - Created cursor
[0m01:14:09.561972 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m01:14:09.565740 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=62d27fad-956b-4b4b-8b3c-12a8bf3b2f0f) - Closing cursor
[0m01:14:09.573667 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.4347827434539795s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Checking idleness
[0m01:14:09.573667 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.4347827434539795s, acquire-count=1, language=None, thread-identifier=(9212, 12600), compute-name=) - Retrieving connection
[0m01:14:09.573667 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m01:14:09.573667 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m01:14:09.579689 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=Unknown) - Created cursor
[0m01:14:09.793803 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m01:14:09.802978 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, command-id=5210028e-1cff-40f2-9d61-3c3f2e7d793a) - Closing cursor
[0m01:14:09.802978 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355496221328, session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed, name=list_hive_metastore_snapshots, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(9212, 12600), compute-name=) - Released connection
[0m01:14:09.811964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdabe85d-0af9-4eb0-b6af-0de8356b680d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B98A82400>]}
[0m01:14:09.813881 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1355495999472, session-id=None, name=master, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(9212, 3852), compute-name=) - Released connection
[0m01:14:09.815922 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:14:09.815922 [info ] [MainThread]: 
[0m01:14:09.824253 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m01:14:09.824253 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0s, acquire-count=0, language=None, thread-identifier=(9212, 4000), compute-name=) - Creating connection
[0m01:14:09.832353 [debug] [Thread-1  ]: Acquiring new databricks connection 'snapshot.medallion_dbt_spark.address_snapshot'
[0m01:14:09.833710 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`address_snapshot`'
[0m01:14:09.833710 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m01:14:09.857709 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m01:14:09.862260 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.863785 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.866793 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m01:14:09.866793 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m01:14:09.866793 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.address_snapshot, idle-time=0.003008127212524414s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:09.866793 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m01:14:09.872901 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.008550882339477539s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.address_snapshot
[0m01:14:09.875075 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.010816097259521484s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`customer_snapshot`'
[0m01:14:09.875075 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m01:14:09.883965 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m01:14:09.892526 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.895114 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.00023317337036132812s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.895114 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m01:14:09.898657 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m01:14:09.899063 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customer_snapshot, idle-time=0.004182100296020508s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:09.899063 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m01:14:09.903174 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0072252750396728516s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customer_snapshot
[0m01:14:09.904672 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.009791374206542969s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`customeraddress_snapshot`'
[0m01:14:09.904672 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m01:14:09.915142 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m01:14:09.915142 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.915142 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.922188 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m01:14:09.924546 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m01:14:09.924546 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.customeraddress_snapshot, idle-time=0.009403467178344727s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:09.924546 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m01:14:09.924546 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.009403467178344727s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m01:14:09.931120 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.009403467178344727s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`product_snapshot`'
[0m01:14:09.932156 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m01:14:09.942845 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m01:14:09.944236 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.944236 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.948150 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m01:14:09.948150 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m01:14:09.948150 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.product_snapshot, idle-time=0.0039136409759521484s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:09.952756 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m01:14:09.952756 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.008520364761352539s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.product_snapshot
[0m01:14:09.952756 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.008520364761352539s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`productmodel_snapshot`'
[0m01:14:09.956327 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m01:14:09.966742 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m01:14:09.966742 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.966742 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.972456 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m01:14:09.973655 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m01:14:09.973655 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.productmodel_snapshot, idle-time=0.006913661956787109s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:09.973655 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m01:14:09.973655 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.006913661956787109s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.productmodel_snapshot
[0m01:14:09.980707 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.013965606689453125s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderdetail_snapshot`'
[0m01:14:09.982254 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m01:14:09.992052 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m01:14:09.993118 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.993118 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:09.997893 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m01:14:09.997893 [debug] [Thread-1  ]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m01:14:09.997893 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle-time=0.004774808883666992s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:10.002985 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m01:14:10.002985 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.009866952896118164s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m01:14:10.005544 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.012425661087036133s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`snapshots`.`salesorderheader_snapshot`'
[0m01:14:10.005544 [debug] [Thread-1  ]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m01:14:10.014644 [debug] [Thread-1  ]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m01:14:10.014644 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.021574 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.023653 [debug] [Thread-1  ]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m01:14:10.024434 [debug] [Thread-1  ]: Began running node model.medallion_dbt_spark.dim_customer
[0m01:14:10.024434 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle-time=0.0028591156005859375s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:10.024434 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderheader_snapshot, now model.medallion_dbt_spark.dim_customer)
[0m01:14:10.024434 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0028591156005859375s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m01:14:10.024434 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0028591156005859375s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_customer`'
[0m01:14:10.032480 [debug] [Thread-1  ]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m01:14:10.046575 [debug] [Thread-1  ]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m01:14:10.046575 [debug] [Thread-1  ]: Began executing node model.medallion_dbt_spark.dim_customer
[0m01:14:10.046575 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.053271 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.054608 [debug] [Thread-1  ]: Finished running node model.medallion_dbt_spark.dim_customer
[0m01:14:10.054608 [debug] [Thread-1  ]: Began running node model.medallion_dbt_spark.dim_product
[0m01:14:10.054608 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_customer, idle-time=0.001337289810180664s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:10.054608 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m01:14:10.062277 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.001337289810180664s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_customer
[0m01:14:10.062823 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.009551286697387695s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`saleslt`.`dim_product`'
[0m01:14:10.062823 [debug] [Thread-1  ]: Began compiling node model.medallion_dbt_spark.dim_product
[0m01:14:10.074617 [debug] [Thread-1  ]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m01:14:10.074617 [debug] [Thread-1  ]: Began executing node model.medallion_dbt_spark.dim_product
[0m01:14:10.074617 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.074617 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.082706 [debug] [Thread-1  ]: Finished running node model.medallion_dbt_spark.dim_product
[0m01:14:10.082706 [debug] [Thread-1  ]: Began running node model.medallion_dbt_spark.sales
[0m01:14:10.082706 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.dim_product, idle-time=0.008088827133178711s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Checking idleness
[0m01:14:10.082706 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.sales)
[0m01:14:10.082706 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.008088827133178711s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Reusing connection previously named model.medallion_dbt_spark.dim_product
[0m01:14:10.082706 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.008088827133178711s, acquire-count=1, language=sql, thread-identifier=(9212, 4000), compute-name=) - Acquired connection on thread (9212, 4000), using default compute resource for model '`hive_metastore`.`saleslt`.`sales`'
[0m01:14:10.091143 [debug] [Thread-1  ]: Began compiling node model.medallion_dbt_spark.sales
[0m01:14:10.103197 [debug] [Thread-1  ]: Writing injected SQL for node "model.medallion_dbt_spark.sales"
[0m01:14:10.103197 [debug] [Thread-1  ]: Began executing node model.medallion_dbt_spark.sales
[0m01:14:10.103197 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.107823 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=1355497381648, session-id=None, name=model.medallion_dbt_spark.sales, idle-time=0.0s, acquire-count=0, language=sql, thread-identifier=(9212, 4000), compute-name=) - Released connection
[0m01:14:10.107823 [debug] [Thread-1  ]: Finished running node model.medallion_dbt_spark.sales
[0m01:14:10.114163 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:14:10.114163 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m01:14:10.116507 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m01:14:10.116507 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:14:10.116507 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m01:14:10.116507 [debug] [MainThread]: Databricks adapter: Connection(session-id=37ddc19d-c84d-4c61-81b7-5ddbc228a8ed) - Closing connection
[0m01:14:10.206246 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.sales' was properly closed.
[0m01:14:10.214130 [debug] [MainThread]: Command end result
[0m01:14:11.368731 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1357189933904, session-id=None, name=generate_catalog, idle-time=0s, acquire-count=0, language=None, thread-identifier=(9212, 3852), compute-name=) - Creating connection
[0m01:14:11.372270 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m01:14:11.372825 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1357189933904, session-id=None, name=generate_catalog, idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 3852), compute-name=) - Acquired connection on thread (9212, 3852), using default compute resource
[0m01:14:11.374311 [info ] [MainThread]: Building catalog
[0m01:14:11.382958 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0s, acquire-count=0, language=None, thread-identifier=(9212, 4680), compute-name=) - Creating connection
[0m01:14:11.382958 [debug] [ThreadPool]: Acquiring new databricks connection '('hive_metastore', 'saleslt')'
[0m01:14:11.384519 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Acquired connection on thread (9212, 4680), using default compute resource
[0m01:14:11.392177 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Checking idleness
[0m01:14:11.392928 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.00840902328491211s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Retrieving connection
[0m01:14:11.394276 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.00975656509399414s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Checking idleness
[0m01:14:11.394276 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=None, name=('hive_metastore', 'saleslt'), idle-time=0.00975656509399414s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Retrieving connection
[0m01:14:11.394276 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:14:11.394276 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'saleslt')"
[0m01:14:11.394276 [debug] [ThreadPool]: On ('hive_metastore', 'saleslt'): /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'saleslt')"} */

      select current_catalog()
  
[0m01:14:11.400559 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:14:11.702721 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Connection created
[0m01:14:11.702721 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=Unknown) - Created cursor
[0m01:14:11.924572 [debug] [ThreadPool]: SQL status: OK in 0.5199999809265137 seconds
[0m01:14:11.924572 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=294cb63b-a9a5-408f-95c0-200f2f57f03c) - Closing cursor
[0m01:14:11.942797 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'saleslt'), idle-time=0.24007558822631836s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Checking idleness
[0m01:14:11.948801 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'saleslt'), idle-time=0.2460799217224121s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Retrieving connection
[0m01:14:11.948801 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'saleslt')"
[0m01:14:11.948801 [debug] [ThreadPool]: On ('hive_metastore', 'saleslt'): /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'saleslt')"} */
show table extended in `hive_metastore`.`saleslt` like 'salesorderheader|address|productmodel|salesorderdetail|dim_customer|product|productdescription|sales|productcategory|customeraddress|customer|dim_product'
  
[0m01:14:11.952351 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=Unknown) - Created cursor
[0m01:14:12.805071 [debug] [ThreadPool]: SQL status: OK in 0.8500000238418579 seconds
[0m01:14:12.812686 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=f7088fba-9d6e-41f8-9a64-500e673df236) - Closing cursor
[0m01:14:12.833045 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'saleslt'), idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(9212, 4680), compute-name=) - Released connection
[0m01:14:12.837602 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'saleslt'), idle-time=0.004557132720947266s, acquire-count=0, language=None, thread-identifier=(9212, 4680), compute-name=) - Checking idleness
[0m01:14:12.840122 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly ('hive_metastore', 'saleslt'), now ('hive_metastore', 'snapshots'))
[0m01:14:12.842697 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.00965261459350586s, acquire-count=0, language=None, thread-identifier=(9212, 4680), compute-name=) - Reusing connection previously named ('hive_metastore', 'saleslt')
[0m01:14:12.844255 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.011209964752197266s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Acquired connection on thread (9212, 4680), using default compute resource
[0m01:14:12.852807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.019219636917114258s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Checking idleness
[0m01:14:12.852807 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.019762039184570312s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Retrieving connection
[0m01:14:12.852807 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'snapshots')"
[0m01:14:12.856362 [debug] [ThreadPool]: On ('hive_metastore', 'snapshots'): /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'snapshots')"} */

      select current_catalog()
  
[0m01:14:12.856362 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=Unknown) - Created cursor
[0m01:14:13.056629 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m01:14:13.064349 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=91d14567-8342-433a-846c-96080d8e1370) - Closing cursor
[0m01:14:13.073173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.23952054977416992s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Checking idleness
[0m01:14:13.073173 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.2401282787322998s, acquire-count=1, language=None, thread-identifier=(9212, 4680), compute-name=) - Retrieving connection
[0m01:14:13.073173 [debug] [ThreadPool]: Using databricks connection "('hive_metastore', 'snapshots')"
[0m01:14:13.073173 [debug] [ThreadPool]: On ('hive_metastore', 'snapshots'): /* {"app": "dbt", "dbt_version": "1.8.4", "dbt_databricks_version": "1.8.4", "databricks_sql_connector_version": "3.1.2", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "('hive_metastore', 'snapshots')"} */
show table extended in `hive_metastore`.`snapshots` like 'address_snapshot|salesorderheader_snapshot|productmodel_snapshot|customer_snapshot|product_snapshot|customeraddress_snapshot|salesorderdetail_snapshot'
  
[0m01:14:13.073173 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=Unknown) - Created cursor
[0m01:14:13.987202 [debug] [ThreadPool]: SQL status: OK in 0.9100000262260437 seconds
[0m01:14:13.994293 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, command-id=bacb58e9-ece8-43aa-a6f8-f5fcf895d9d5) - Closing cursor
[0m01:14:14.002965 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=1355497983568, session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa, name=('hive_metastore', 'snapshots'), idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(9212, 4680), compute-name=) - Released connection
[0m01:14:14.012748 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=1357189933904, session-id=None, name=generate_catalog, idle-time=0.0s, acquire-count=0, language=None, thread-identifier=(9212, 3852), compute-name=) - Released connection
[0m01:14:14.145590 [info ] [MainThread]: Catalog written to C:\Medallion-Spark-DBT\medallion_dbt_spark\target\catalog.json
[0m01:14:14.145590 [debug] [MainThread]: Command `dbt docs generate` succeeded at 01:14:14.145590 after 11.97 seconds
[0m01:14:14.145590 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m01:14:14.153039 [debug] [MainThread]: Connection '('hive_metastore', 'snapshots')' was properly closed.
[0m01:14:14.154083 [debug] [MainThread]: On ('hive_metastore', 'snapshots'): ROLLBACK
[0m01:14:14.154083 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:14:14.154083 [debug] [MainThread]: On ('hive_metastore', 'snapshots'): Close
[0m01:14:14.154083 [debug] [MainThread]: Databricks adapter: Connection(session-id=c29095d7-dda3-433a-a19c-13e4eff2deaa) - Closing connection
[0m01:14:14.243056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFC4EDDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013B9948B670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BFCA19580>]}
[0m01:14:14.243056 [debug] [MainThread]: Flushing usage events
[0m01:14:55.772701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183770ECE20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183797BB760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183797BB5B0>]}


============================== 01:14:55.772701 | 1f901e5a-88d0-4c21-80a3-80cca52d0023 ==============================
[0m01:14:55.772701 [info ] [MainThread]: Running with dbt=1.8.4
[0m01:14:55.785134 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\User\\.dbt', 'log_path': 'C:\\Medallion-Spark-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt docs serve', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:14:55.983675 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:14:55.983675 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:14:55.983675 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:14:58.769443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1f901e5a-88d0-4c21-80a3-80cca52d0023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183797A4520>]}
[0m01:14:58.884547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1f901e5a-88d0-4c21-80a3-80cca52d0023', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018378B3CD90>]}
